---
title: "Data science ethics"
subtitle: "Lecture 13"
date: "June 4, 2024"
format: 
  revealjs:
    self-contained: true
    footer: "[🔗 sta199-summer24.github.io](https://sta199-s24.github.io/) &nbsp;·&nbsp; [❓ Ask on Ed](https://edstem.org/us/courses/59071/discussion/)"
execute: 
  echo: true
editor_options: 
  chunk_output_type: console
---


```{r}
#| echo: false
#| message: false

library(countdown)
library(tidyverse)
ggplot2::theme_set(theme_gray(base_size = 16))
```




## Today

Data science ethics:

-   Misrepresentation

-   Data privacy

-   Algorithmic bias

# Misrepresentation

## Stand your ground {.smaller}

::: question
In 2005, the [*Florida legislature*](https://en.wikipedia.org/w/index.php?search=Florida%20legislature) passed the controversial ["Stand Your Ground" law](https://en.wikipedia.org/wiki/Stand-your-ground_law) that broadened the situations in which citizens can use lethal force to protect themselves against perceived threats.
Advocates believed that the new law would ultimately reduce crime; opponents feared an increase in the use of lethal force.
:::

## Stand your ground {.smaller}

::: panel-tabset
## Question

::: question
What does the visualization, published by [Reuters](http://static5.businessinsider.com/image/53038b556da8110e5ce82be7-604-756/florida%20gun%20deaths.jpg) on February 16, 2014, say about the number of firearm murders in Florida over the years?
:::

![](images/stand-your-ground.png){fig-align="center" width="711"}

```{r}
#| echo: false

countdown(minutes = 3, left = 0, bottom = 0.1)
```


:::

# Data privacy

## "Your" data

-   Every time we use apps, websites, and devices, our data is being collected and used or sold to others.

-   More importantly, decisions are made by law enforcement, financial institutions, and governments based on data that directly affect the lives of people.

## Privacy of your data

::: question
What pieces of data have you left on the internet today?
Think through everything you've logged into, clicked on, checked in, either actively or automatically, that might be tracking you.
Do you know where that data is stored?
Who it can be accessed by?
Whether it's shared with others?
:::

## Sharing your data {.smaller}

::: question
What are you OK with sharing?
:::

::: columns
::: {.column width="50%"}
::: incremental
-   Name
-   Age
-   Email
-   Phone Number
-   List of every video you watch
-   List of every video you comment on
:::
:::

::: {.column width="50%"}
::: incremental
-   How you type: speed, accuracy
-   How long you spend on different content
-   List of all your private messages (date, time, person sent to)
-   Info about your photos (how it was taken, where it was taken (GPS), when it was taken)
:::
:::
:::

## What does Google think/know about you?

::: question
Have you ever thought about why you're seeing an ad on Google?
Google it!
Try to figure out if you have ad personalization on and how your ads are personalized.
:::

```{r}
#| echo: false

countdown(minutes = 3)
```

## Your browing history

::: question
Which of the following are you OK with your browsing history to be used towards?
:::

::: incremental
-   For serving you targeted ads
-   To score you as a candidate for a job
-   To predict your race/ethnicity for voting purposes
:::

## Who else gets to use your data?

::: question
Suppose you create a profile on a social media site and share your personal information on your profile.
Who else gets to use that data?
:::

::: incremental
-   Companies the social media company has a connection to?
-   Companies the social media company sells your data to?
-   Researchers?
:::

## OK Cupid data breach {.xsmall}

-   In 2016, researchers published data of 70,000 OkCupid users---including usernames, political leanings, drug usage, and intimate sexual details
-   Researchers didn't release the real names and pictures of OKCupid users, but their identities could easily be uncovered from the details provided, e.g. usernames

. . .

::: columns
::: {.column width="60%"}
> Some may object to the ethics of gathering and releasing this data.
> However, all the data found in the dataset are or were already publicly available, so releasing this dataset merely presents it in a more useful form.
>
> Researchers Emil Kirkegaard and Julius Daugbjerg Bjerrekær
:::

::: {.column width="40%"}
![](images/okcupid-tweet.png){fig-align="center" width="500"}
:::
:::

# Algorithmic bias


## Garbage in, garbage out

-   In statistical modeling and inference we talk about "garbage in, garbage out" -- if you don't have good (random, representative) data, results of your analysis will not be reliable or generalizable.
-   Corollary: Bias in, bias out.

## Google translate

::: question
What might be the reason for Google's gendered translation?
How do ethics play into this situation?
:::

![](images/google-translate.png){fig-align="center"}

::: aside
Source: [Engadget - Google is working to remove gender bias in its translations](https://www.engadget.com/2018-12-07-google-remove-gender-bias-translations.html)
:::

# Stochastic Parrots

## What is meant by “stochastic parrots” in the paper title?

[On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? 🦜](https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf) (Bender et. al., 2021)


```{r}
#| echo: false

countdown(minutes = 5)
```


## Machine Bias {.smaller}

2016 ProPublica article on algorithm used for rating a defendant's risk of future crime:

::: columns
::: {.column width="70%"}
> In forecasting who would re-offend, the algorithm made mistakes with black and white defendants at roughly the same rate but in very different ways.
>
> -   The formula was particularly likely to falsely flag black defendants as future criminals, wrongly labeling them this way at almost twice the rate as white defendants.
>
> -   White defendants were mislabeled as low risk more often than black defendants.
:::

::: {.column width="30%"}
![](images/machine-bias.png){fig-align="center" width="800"}
:::
:::

::: aside
Source: [ProPublica](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)
:::

## Risk score errors

::: columns
::: {.column width="35%"}
::: question
What is common among the defendants who were assigned a high/low risk score for reoffending?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="30%"}
![](images/machine-bias-petty-theft-1.png){fig-align="center" width="300" height="250"} ![](images/machine-bias-petty-theft-2.png){fig-align="center" width="300" height="180"}
:::

::: {.column width="30%"}
![](images/machine-bias-drug-posession-1.png){fig-align="center" width="300" height="250"} ![](images/machine-bias-drug-posession-2.png){fig-align="center" width="300" height="180"}
:::
:::

## Risk scores

::: columns
::: {.column width="35%"}
::: question
How can an algorithm that doesn't use race as input data be racist?
:::
:::

::: {.column width="5%"}
:::

::: {.column width="60%"}
![](images/machine-bias-risk-scores.png){fig-align="center" width="480"}
:::
:::

